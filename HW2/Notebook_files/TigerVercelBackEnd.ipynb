{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Backend Section - Crawler**"
      ],
      "metadata": {
        "id": "RPpznh6WV2gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "MSAmnzvuV7V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installations and imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "!pip install firebase\n",
        "from firebase import firebase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_VJGHB-V1rQ",
        "outputId": "89ca3747-1d3e-4bb9-8056-210dd6490661"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting firebase\n",
            "  Downloading firebase-4.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firebase) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (2024.12.14)\n",
            "Downloading firebase-4.0.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: firebase\n",
            "Successfully installed firebase-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dictionary creating function**"
      ],
      "metadata": {
        "id": "2qiyN-qWWJ9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count words from page\n",
        "def create_count_dictionary(words):\n",
        "  dictionary = defaultdict(int)\n",
        "  print(\"--> create_count_dictionary\",words)\n",
        "  for word in words:\n",
        "    if word not in dictionary:\n",
        "      dictionary[word] = 1\n",
        "    else:\n",
        "      dictionary[word] += 1\n",
        "\n",
        "  #print(dictionary.keys())\n",
        "\n",
        "  #for key in dictionary:\n",
        "  #  print(key, \": \", dictionary[key])\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "fXJBiBTSWGkS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crawler data upload to database function**"
      ],
      "metadata": {
        "id": "IaJurqz6WNcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_data_to_DB(dictionary, url):\n",
        "  print(\"--> upload_data_to_DB\")\n",
        "  data_dict = dict(dictionary)\n",
        "  length = len(data_dict)\n",
        "  #i = 0\n",
        "  FBconn = firebase.FirebaseApplication('https://vercelcrawler-1c167-default-rtdb.firebaseio.com/',None)\n",
        "\n",
        "  for key in data_dict:\n",
        "      clean_key = re.sub(r'[^\\w\\s]', ' ', key) # TODO: is this still neccesery\n",
        "\n",
        "      # First, get existing data if any\n",
        "      existing_data = FBconn.get('/', clean_key) # get any existing datawith this term\n",
        "\n",
        "      if existing_data is None: # If no existing data, create new structure\n",
        "          upload_data = {\n",
        "              \"docIDs\": {\n",
        "                  \"0\": {\n",
        "                      \"link\": url,\n",
        "                      \"count\": data_dict[key]\n",
        "                  }\n",
        "              }\n",
        "          }\n",
        "      else: # If existing data found, find the next available docID number\n",
        "          #print(existing_data) # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TODO: maybe add check if page already is in db to replace it (?)\n",
        "          existing_doc_ids = existing_data.get('docIDs', {})\n",
        "          next_id = len(existing_doc_ids)\n",
        "\n",
        "          # Add new data to existing structure\n",
        "          existing_data['docIDs'].append({\n",
        "              \"link\": url,\n",
        "              \"count\": data_dict[key]\n",
        "          })\n",
        "          upload_data = existing_data\n",
        "\n",
        "      # Upload the merged data\n",
        "      FBconn.put('/', clean_key, upload_data)\n",
        "      #i += 1\n",
        "      #print(i,\"/\",length)"
      ],
      "metadata": {
        "id": "cSjv9kp1WJK6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top 10 terms from the website function**"
      ],
      "metadata": {
        "id": "WCRn6iqoWUFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 most repeated words in the site\n",
        "def get_top_terms(term_dict, n=10):\n",
        "    \"\"\"\n",
        "    Returns the n most frequent terms and their counts from a dictionary.\n",
        "\n",
        "    Args:\n",
        "        term_dict (dict): Dictionary with terms as keys and counts as values\n",
        "        n (int): Number of top terms to return (default 10)\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples [(term, count), ...] sorted by count in descending order\n",
        "    \"\"\"\n",
        "    # Sort dictionary items by value (count) in descending order\n",
        "    sorted_terms = sorted(term_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return first n items (or all if dict is smaller than n)\n",
        "    return sorted_terms[:n]\n",
        "\n",
        "# avg repetitions per word\n",
        "\n",
        "def calculate_avg(term_dict):\n",
        "  sum = np.sum(list(term_dict.values()))\n",
        "  cnt = len(term_dict)\n",
        "  return round(sum/cnt,2)\n",
        "\n",
        "# Action vs Restriction words\n",
        "\n",
        "def calculate_action_restriction_percentages(term_dict):\n",
        "    action_terms = [\n",
        "        \"build\", \"deploy\", \"create\", \"configure\", \"integrate\", \"manage\", \"scale\",\n",
        "        \"test\", \"update\", \"connect\", \"provision\", \"launch\", \"clone\", \"allocate\",\n",
        "        \"enable\", \"query\", \"retrieve\", \"replicate\", \"execute\", \"optimize\", \"monitor\",\n",
        "        \"share\", \"collaborate\", \"contribute\", \"invite\", \"access\"\n",
        "    ]\n",
        "    restrictive_terms = [\n",
        "        \"prohibit\", \"terminate\", \"restrict\", \"limit\", \"revoke\", \"suspend\", \"forbid\",\n",
        "        \"deny\", \"disallow\", \"cease\", \"throttle\", \"cap\", \"exceed\", \"disable\", \"block\",\n",
        "        \"prevent\", \"restrictapi\", \"limitbandwidth\", \"resourcecap\", \"liable\",\n",
        "        \"nontransferable\", \"breach\", \"violation\", \"infringement\", \"indemnify\"\n",
        "    ]\n",
        "\n",
        "    # Convert lists to sets for faster lookup\n",
        "    action_set = set(action_terms)\n",
        "    restrictive_set = set(restrictive_terms)\n",
        "\n",
        "    sum_total = np.sum(list(term_dict.values()))\n",
        "\n",
        "    # Handle edge case of empty dictionary or zero sum\n",
        "    if sum_total == 0:\n",
        "        return [0.0, 0.0]\n",
        "\n",
        "    action_count = 0\n",
        "    restriction_count = 0\n",
        "\n",
        "    for term, count in term_dict.items():\n",
        "        if term in action_set:\n",
        "            action_count += count\n",
        "        if term in restrictive_set:\n",
        "            restriction_count += count\n",
        "\n",
        "    action_per = round((action_count/sum_total) * 100, 2)\n",
        "    restriction_per = round((restriction_count/sum_total) * 100, 2)\n",
        "\n",
        "    return [action_per, restriction_per]\n"
      ],
      "metadata": {
        "id": "JjAIhUB2WZa4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crawler main code: Crawl main page and upload to Firebase**"
      ],
      "metadata": {
        "id": "w_ziLtStXJXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "visited_links = set()  # saving a set of already visited links\n",
        "word_count = 0\n",
        "base_url = \"https://vercel.com/home\"\n",
        "def crawl(url):\n",
        "    global word_count  # TODO: check why dis global\n",
        "\n",
        "    try:\n",
        "        FBconn = firebase.FirebaseApplication('https://vercelcrawler-1c167-default-rtdb.firebaseio.com/',None)\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            return\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        entire_text = preprocess_text(soup.get_text())\n",
        "        dictionary = create_count_dictionary(entire_text)\n",
        "        upload_data_to_DB(dictionary,url)\n",
        "        top_ten = get_top_terms(dictionary)\n",
        "        avg = calculate_avg(dictionary)\n",
        "        act_vs_rest = calculate_action_restriction_percentages(dictionary)\n",
        "        upload_data = {\n",
        "            \"_greatest10\": top_ten,\n",
        "            \"_avg_term_appearance\": avg,\n",
        "            \"_action_vs_restriction\": act_vs_rest\n",
        "        }\n",
        "        FBconn.put('/', '_statistics', upload_data)\n",
        "        print(\"FINISHED!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n",
        "    return entire_text\n",
        "\n",
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: string\n",
        "  Returns:\n",
        "    list of preprocessed words\n",
        "  \"\"\"\n",
        "  print(\"--> preproccess_text\")\n",
        "  # List of english stop words\n",
        "  STOP_WORDS = set(stopwords.words('english'))\n",
        "                                            #print(\"---> Original text: \", text)\n",
        "  # Remove punctuation\n",
        "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "                                            #print(\"---> Remove punctuation: \", text)\n",
        "  # split camelcase and pascalcase\n",
        "  # Add space before capital letters that aren't at the start\n",
        "  text = re.sub(r'(?<!^)(?<![\\W\\d_])([A-Z])', r' \\1', text)\n",
        "                                            #print(\"---> Remove camelPascal: \", text)\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "  # Split between digits and letters (both directions)\n",
        "  text = re.sub(r'(\\d+)([a-z])', r'\\1 \\2', text)  # digits followed by letters\n",
        "  text = re.sub(r'([a-z])(\\d+)', r'\\1 \\2', text)  # letters followed by digits\n",
        "                                            #print(\"---> Convert to lowercased: \", text)\n",
        "  # Tokenize\n",
        "  words = nltk.word_tokenize(text)\n",
        "                                            #print(\"---> Tokenize: \", words)\n",
        "  # Remove stop words\n",
        "  words = [word for word in words if word not in STOP_WORDS]\n",
        "                                            #print(\"---> Remove stop words: \", words)\n",
        "  # Lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "                                            #print(\"---> Lemmatize: \", lemmas)\n",
        "  return lemmas\n",
        "\n",
        "# start crawler in first page\n",
        "words = crawl(base_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YQYsHpcXIew",
        "outputId": "5dea832e-00ab-49ed-a4bf-dcb496100796"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> preproccess_text\n",
            "--> create_count_dictionary ['vercel', 'build', 'deploy', 'best', 'web', 'experience', 'frontend', 'cloud', 'product', 'x', 'platform', 'preview', 'helping', 'team', 'ship', '6', 'faster', 'powering', 'breakthrough', 'managed', 'infrastructure', 'rendering', 'fast', 'scalable', 'reliable', 'observability', 'trace', 'every', 'step', 'security', 'scale', 'without', 'compromising', 'open', 'source', 'next', 'j', 'native', 'next', 'j', 'platform', 'turborepo', 'speed', 'enterprise', 'scale', 'k', 'toolkit', 'type', 'script', 'solution', 'use', 'case', 'apps', 'deploy', 'speed', 'composable', 'commerce', 'power', 'storefront', 'convert', 'marketing', 'site', 'launch', 'campaign', 'fast', 'multi', 'tenant', 'platform', 'scale', 'apps', 'one', 'codebase', 'web', 'apps', 'ship', 'feature', 'infrastructure', 'user', 'platform', 'engineer', 'automate', 'away', 'repetition', 'design', 'engineer', 'deploy', 'every', 'idea', 'resource', 'tool', 'resource', 'center', 'today', 'best', 'practice', 'marketplace', 'extend', 'automate', 'workflow', 'template', 'jumpstart', 'app', 'development', 'guide', 'find', 'help', 'quickly', 'partner', 'finder', 'get', 'help', 'solution', 'partner', 'company', 'customer', 'trusted', 'best', 'team', 'blog', 'latest', 'post', 'change', 'changelog', 'see', 'shipped', 'press', 'read', 'latest', 'news', 'enterprise', 'doc', 'pricing', 'log', 'contact', 'sign', 'sign', 'complete', 'platform', 'web', 'vercel', 'provides', 'developer', 'tool', 'cloud', 'infrastructure', 'build', 'scale', 'secure', 'faster', 'personalized', 'web', 'deploy', 'start', 'deploying', 'get', 'demo', 'complete', 'platform', 'web', 'vercel', 'provides', 'developer', 'tool', 'cloud', 'infrastructure', 'build', 'scale', 'secure', 'faster', 'personalized', 'web', 'deploy', 'start', 'deploying', 'get', 'demo', 'build', 'time', 'went', '7', '40', 'saw', '95', 'reduction', 'page', 'load', 'time', 'saw', '24', 'x', 'faster', 'build', 'apps', 'apps', 'web', 'apps', 'web', 'apps', 'ecommerce', 'ecommerce', 'marketing', 'marketing', 'platform', 'platform', 'get', 'started', 'using', 'pre', 'built', 'template', 'easily', 'stream', 'long', 'running', 'l', 'l', 'response', 'better', 'user', 'experience', 'zero', 'config', 'infrastructure', 'always', 'globally', 'performant', 'fast', 'load', 'time', 'zero', 'overhead', 'vercel', 'highly', 'optimized', 'infrastructure', 'c', 'n', 'reducing', 'bounce', 'rate', 'improving', 'e', 'streamline', 'content', 'creation', 'publishing', 'built', 'preview', 'deploy', 'apps', 'second', 'git', 'connected', 'deploys', 'localhost', 'http', 'second', 'deploy', 'git', 'c', 'l', 'vercel', 'site', 'git', 'push', 'enumerating', 'object', '1', 'done', 'counting', 'object', '100', '1', '1', 'done', 'writing', 'object', '100', '1', '1', '72', 'byte', 'done', 'total', '1', 'delta', '0', 'reused', '0', 'delta', '0', 'github', 'com', 'vercel', 'vercel', 'site', 'git', '21326', '9', '81663', 'c', '3', 'main', 'mainvercel', 'com', 'ship', 'collaborative', 'pre', 'production', 'every', 'deploy', 'remarkable', 'chat', 'team', 'real', 'production', 'grade', 'u', 'design', 'swapped', 'button', 'variant', 'needed', 'pranathi', 'instead', 'like', 'work', 'brand', 'tweak', 'mamuso', 'rauno', 'look', 'great', 'frontend', 'observability', 'privacy', 'friendly', 'lightweight', 'analytics', 'upgrade', 'post', 'launch', 'workflow', 'actionable', 'real', 'time', 'insight', '5', '0004', '0003', '0002', '0001', '000', 'view', '7000', '90', 'click', '3500', '90', '7', '0006', '0005', '0004', '0003', '0002', '0001', '000', 'view', '3981', '46', 'click', '2270', '51', 'instant', 'rollback', 'go', 'ahead', 'deploy', 'friday', 'instantly', 'rollback', 'working', 'deployment', 'vercel', 'site', 'jvjb', '4', 'ynna', '1', 'agoba', '5', 'f', '55', 'f', 'update', 'bento', 'update', 'bento', 'box', 'design', '90', 'vercel', 'site', 'gigj', '178', 'pv', '10', 'agobx', '012', 'mm', 'fix', 'e', 'lint', 'error', 'fix', 'e', 'lint', 'error', 'query', '55', 'conformance', 'move', 'fast', 'break', 'thing', 'keep', 'quality', 'high', 'maintaining', 'velocity', 'enterprise', 'monorepos', 'conformance', 'excellent', '9', '5', 'total', 'issue', '34', 'major', 'issue', '12', 'code', 'owner', 'vercel', 'design', 'vercel', 'eng', 'vercel', 'marketing', 'scale', 'enterprisewithout', 'compromising', 'security', 'deploy', 'deliver', 'everywhere', 'push', 'code', 'vercel', 'make', 'instantly', 'available', 'across', 'planet', 'infrastructure', 'learn', 'enterprise', 'node', 'globe', 'sending', 'small', 'pulse', 'indicate', 'activityapp', 'user', 'page', 'tsxexport', 'default', 'async', 'function', 'page', 'const', 'data', 'await', 'sql', 'e', 'l', 'e', 'c', 'u', 'e', 'r', 'return', 'h', '1', 'user', 'h', '1', 'ul', 'data', 'map', 'user', 'li', 'key', 'user', 'id', 'user', 'name', 'li', 'ul', 'bridging', 'best', 'client', 'server', 'rendering', 'experience', 'seamless', 'integration', 'optimized', 'performance', 'reduced', 'bundle', 'size', 'like', 'never', 'rendering', 'data', 'connect', 'content', 'commerce', 'database', 'platform', 'discover', 'integration', 'framework', 'way', 'framework', 'way', 'reliability', 'count', 'elastic', 'scalability', 'handle', 'unbelievable', 'scale', 'without', 'sweat', 'whether', 'fortune', '500', 'launch', 'day', 'rock', 'solid', 'security', 'infrastructure', 'designed', 'automatically', 'mitigate', 'attack', 'protect', 'information', 'global', 'performance', 'automatically', 'route', 'traffic', '100', 'edge', 'location', 'around', 'globe', 'fast', 'site', 'anywhere', 'world', 'user', 'first', 'protection', 'vercel', 'automatically', 'cache', 'site', 'ensure', 'even', 'backend', 'service', 'go', 'site', 'stay', 'serverless', 'storage', 'accelerate', 'development', 'database', 'fastest', 'frontends', '99', '99', 'uptime', 'deploy', 'first', 'app', 'second', 'deploy', 'automatically', 'git', 'c', 'l', 'wide', 'range', 'support', 'popular', 'framework', 'preview', 'every', 'push', 'automatic', 'h', 'p', 'domain', 'next', 'j', 'template', 'react', 'template', 'astro', 'template', 'svelte', 'template', 'nuxt', 'template', 'python', 'template', 'start', 'deploying', 'start', 'deploying', 'talk', 'sale', 'team', 'talk', 'sale', 'get', 'enterprise', 'trial', 'enterprise', 'trial', 'product', 'enterprise', 'next', 'j', 'observability', 'preview', 'rendering', 'security', 'turbov', '0', 'resource', 'community', 'doc', 'expert', 'guide', 'help', 'integration', 'pricing', 'resource', 'template', 'company', 'blog', 'career', 'changelog', 'contact', 'u', 'customer', 'partner', 'privacy', 'policy', 'legal', 'social', 'git', 'hub', 'linked', 'twitter', 'tube', 'loading', 'status', 'select', 'display', 'theme', 'system', 'lightdark']\n",
            "--> upload_data_to_DB\n",
            "FINISHED!\n"
          ]
        }
      ]
    }
  ]
}